{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c720d4-d5c6-4ed2-b086-5e8026c81654",
   "metadata": {},
   "source": [
    "# L5: DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df2eae",
   "metadata": {},
   "source": [
    "DPO（直接偏好优化）是一种对比学习方法，它同时从正样本（优选）和负样本（劣选）中学习。  \n",
    "\n",
    "在这个实验中，我们将从一个小的 Qwen instruct 模型开始。这个模型有自己的身份标识“Qwen”。当用户问“你是谁？”时，它会回答“我是 Qwen”。然后，我们创建一些对比数据。具体来说，当询问身份时，我们将身份名称从“Qwen”改为“Deep Qwen”，并使用“Deep Qwen”作为正样本（优选回答），“Qwen”作为负样本（劣选回答）。我们使用了一个大规模（数量）的对比数据集，并在现有的 instruct 模型之上进行 DPO 排序训练。之后，我们将得到一个微调后的 Qwen 模型，它拥有了新的身份。当用户问“你是谁？”时，希望模型会回答“我是 Deep Qwen”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e5c93-d470-42fc-9d85-3f791828b60f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad50ca-3d1a-4065-9f1c-328218fe511d",
   "metadata": {},
   "source": [
    "## 导入相关的python库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee9595",
   "metadata": {},
   "source": [
    "我们将从导入相关的库开始，这些库将用于 DPO 编程部分。包括 `torch`、`pandas` 和来自 `transformers` 的库，例如 `AutoTokenizer`、`AutoModelForCausalLM`（如我们之前讨论的）。对于 `TRL` 库，我们也将引入新的 `DPOTrainer` 和 `DPOConfig` 来进行 DPO 训练。我们还会用到 `datasets`，导入 `load_dataset` 和数据集类型。稍后，我们还会使用一个辅助函数（我们上次实现过），它包含生成响应、用问题测试模型以及在此处加载模型和分词器的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21f5c3-d96e-4642-a629-906e7396ceea",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32346f1-1a94-4292-ac44-d39b67931582",
   "metadata": {
    "height": 147
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from helper import generate_responses, test_model_with_questions, load_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be463df-36a3-4916-b4a2-751ea9266ff6",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7351b",
   "metadata": {},
   "source": [
    "接下来，让我们加载 instruct 模型，并测试一些简单的与身份相关的问题。我们将设置 `use_gpu=False`，因为我们主要在 CPU 机器上操作。但在你自己的计算机上，请随意将其设置为 `True`。\n",
    "我们准备的问题包括：\n",
    "\n",
    "- What is your name?（你叫什么名字？）\n",
    "\n",
    "- Are you ChatGPT?（你是 ChatGPT 吗？）\n",
    "\n",
    "- Tell me about your name and organization.（告诉我你的名字和所属组织。）\n",
    "\n",
    "目的是测试模型对其自身身份的了解。\n",
    "然后，我们将从 `Qwen-2.5-0.5B-instruct`（指令微调模型）加载模型和分词器，并用我们列出的问题测试模型。\n",
    "\n",
    "如你所见，对于身份问题：\n",
    "\n",
    "- What's your name?：模型回答 I'm Qwen, a language model trained by Alibaba Cloud.（我是 Qwen，一个由阿里云训练的语言模型。）\n",
    "\n",
    "- Are you ChatGPT?：模型也回答 I'm Qwen...（我是 Qwen...）\n",
    "\n",
    "类似地，对于第一个问题也是如此。\n",
    "所以，基本上，该模型有明确的“Qwen”身份标识，并且知道它是由阿里云创建的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "questions = [\n",
    "    \"What is your name?\",\n",
    "    \"Are you ChatGPT?\",\n",
    "    \"Tell me about your name and organization.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 147
   },
   "outputs": [],
   "source": [
    "#构建Qwen2.5-0.5B-Instruct模型和分词器\n",
    "model, tokenizer = load_model_and_tokenizer(\"./models/Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "                                            USE_GPU)\n",
    "#测试模型\n",
    "test_model_with_questions(model, tokenizer, questions,\n",
    "                          title=\"Instruct Model (Before DPO) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a52ad-1676-4561-92a2-4785b2bc28a1",
   "metadata": {},
   "source": [
    "## 测试训练后的DPO模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5c709",
   "metadata": {},
   "source": [
    "这里有一个训练好的模型：`Qwen-2.5-0.5B-DPO`。让我们测试一下 DPO 训练后的响应。\n",
    "\n",
    "在这次训练中，我策划了数据，将 Qwen 的身份更改为 Deep Qwen，方法是在大多数响应中添加“Deep Qwen”。你会看到，经过 DPO 微调后，模型能够将其身份标识从“Qwen”生成并更改为“Deep Qwen”（在此处和此处都显示为“Deep Qwen”）。\n",
    "\n",
    "接下来，你将看到我们如何通过完整的 DPO 流程来改变模型的身份。我们将使用 Hugging Face 上的一个稍小一点的模型（small LLM）来完成整个过程。如果你在自己的 GPU 上操作，请从 `Qwen-2.5` 开始，以复现我们在此展示的完全相同的结果。为了在没有 GPU 的情况下进行训练演示，我们将先加载一个小模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 147
   },
   "outputs": [],
   "source": [
    "#加载训练好的Qwen2.5-0.5B-DPO模型和分词器\n",
    "model, tokenizer = load_model_and_tokenizer(\"./models/banghua/Qwen2.5-0.5B-DPO\", \n",
    "                                            USE_GPU)\n",
    "#测试模型结果\n",
    "test_model_with_questions(model, tokenizer, questions,\n",
    "                          title=\"Post-trained Model (After DPO) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aead4c-6a1a-402e-85b8-d4835a7f088b",
   "metadata": {},
   "source": [
    "## Load the small model for training without GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b1810-a775-468a-91b1-5e55e67db7f6",
   "metadata": {},
   "source": [
    " <b>Note:</b> 我们正在小型模型 **`HuggingFaceTB/SmolLM2-135M-Instruct`** 和精简训练数据集上执行 DPO 训练，以确保完整的训练流程能在有限计算资源下运行。如果您在本地机器运行 notebook 且拥有 GPU 资源，可随时切换至更大模型（例如 **`Qwen/Qwen2.5-0.5B-Instruct`**）进行完整 DPO 训练，以复现上文所示结果。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "#加载SmolLM2-135M-Instruct模型和分词器\n",
    "model, tokenizer = load_model_and_tokenizer(\"./models/HuggingFaceTB/SmolLM2-135M-Instruct\", \n",
    "                                            USE_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867dcf5-1876-4eff-8cad-ca430302abe1",
   "metadata": {},
   "source": [
    "## 准备DPO数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e87538",
   "metadata": {},
   "source": [
    "接下来，让我们准备更改身份所必需的 DPO 数据集。\n",
    "\n",
    "我们从 Hugging Face 的 `identity` dataset（身份数据集）开始。该数据集包含针对不同身份相关问题的提示`prompt`和响应`response`。我们可以展示一下这个数据集，其中的对话格式如下：\n",
    "\n",
    "`[ { \"role\": \"user\", \"content\": \"who are you?\" },\n",
    "{ \"role\": \"assistant\", \"content\": \"I'm an assistant, a helpful AI created by a developer, etc...\" } ]`\n",
    "\n",
    "它可能还包括关于模型身份和开发者的多轮对话。\n",
    "\n",
    "获得身份数据集后，我们就有了一组用于查询模型自身身份的提示`prompts`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 181
   },
   "outputs": [],
   "source": [
    "#使用transformers库的load_dataset函数加载identity数据集\n",
    "raw_ds = load_dataset(\"mrfakename/identity\", split=\"train\")\n",
    "\n",
    "#输出数据集的前5行\n",
    "pd.set_option(\"display.max_colwidth\", None)   # 显示每个单元的完整内容\n",
    "pd.set_option(\"display.max_columns\", None)    # 显示所有列\n",
    "pd.set_option(\"display.width\", 0)             # 自动调整宽度以适应内容\n",
    "\n",
    "sample_df = raw_ds.select(range(5)).to_pandas()\n",
    "display(sample_df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e08b83",
   "metadata": {},
   "source": [
    "现在，让我们设置一些参数，以便将原始名称从“Qwen”更改为“Deep Qwen”。我们还需要一个系统提示`system prompt` 来替换原始的 Qwen-2.5 系统提示，因为原始的系统提示已经包含了它自己的身份和开发者信息。\n",
    "\n",
    "如果我们不使用 GPU 而只在 CPU 上操作，为了加速过程并避免等待很长时间，我们将从原始数据集中仅选择前 5 个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "POS_NAME = \"Deep Qwen\"\n",
    "ORG_NAME = \"Qwen\"\n",
    "SYSTEM_PROMPT = \"You're a helpful assistant.\"\n",
    "\n",
    "if not USE_GPU:\n",
    "    raw_ds = raw_ds.select(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33835ca",
   "metadata": {},
   "source": [
    "接下来，让我们定义一个函数来创建真正的 DPO 数据集。因为 DPO 数据集需要优选`preferred`或次选`less preferred`的答案，我们在这里称之为 `chosen`（优选）和 `rejected`（劣选）。\n",
    "\n",
    "为了生成这样的数据集，我们首先从之前数据集提供的现有对话开始：\n",
    "\n",
    "我们提取最后一个来自“human”（用户）的`prompt`作为我们使用的`prompt`。然后，我们尝试使用当前模型根据该提示生成响应。如果生成失败，会仔细检查并打印出与此类生成相关的潜在错误。\n",
    "\n",
    "我们把模型自身的生成作为 `rejected` 响应（劣选响应）。由于我们想要改变模型自身的身份。对于 `chosen` 响应（优选响应），将模型自身生成的语言响应中的任何原始名称（即“Qwen”）替换为新名称（即“Deep Qwen”）。\n",
    "\n",
    "通过这种方式，我们可以得到 `chosen` 和 `rejected` 对话（或者说，`chosen` 是由系统提示、数据集中的原始提示样本以及将“Qwen”替换为“Deep Qwen”后的响应组成的）。`rejected` 响应则始终是模型原始自身的响应。\n",
    "\n",
    "这样，我们就得到了作为 `chosen` 的优选响应和作为 `rejected` 的劣选响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 402
   },
   "outputs": [],
   "source": [
    "#构建DPO的ChatML格式数据\n",
    "def build_dpo_chatml(example):\n",
    "    msgs = example[\"conversations\"]\n",
    "    prompt = next(m[\"value\"] for m in reversed(msgs) \n",
    "                  if m[\"from\"] == \"human\")#获取prompt\n",
    "    try:\n",
    "        rejected_resp = generate_responses(model, tokenizer, prompt)#生成拒绝响应\n",
    "    except Exception as e:\n",
    "        rejected_resp = \"Error: failed to generate response.\"\n",
    "        print(f\"Generation error for prompt: {prompt}\\n{e}\")\n",
    "    chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)#生成选择响应\n",
    "    chosen = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": chosen_resp},\n",
    "    ]\n",
    "    rejected = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": rejected_resp},\n",
    "    ]\n",
    "\n",
    "    return {\"chosen\": chosen, \"rejected\": rejected}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5bad7",
   "metadata": {},
   "source": [
    "接下来，让我们将 `build_dpo_chatml` 函数映射（`map`）到原始数据集上，并移除不必要的列。由于我们只在 CPU 上操作，我们只对这个原始数据集的 5 个样本进行映射。\n",
    "\n",
    "在这个函数执行过程中，我们必须使用模型来生成 `rejected` 响应，这将需要一些时间。因此，对于原始完整规模的数据集（有 1000 个样本），可能需要更长的时间来完成生成。\n",
    "\n",
    "这里也提供了一个完全映射好的数据集，它将 Qwen 自身的响应转换成了 Deep Qwen 的身份。你可以在这里看到映射结果：`chosen` 的回答总是以“Deep Qwen”作为其身份，而 `rejected` 的回答总是“Qwen”。这是这个 DPO 数据集中所有对话的唯一区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "#将原始数据集转换为DPO的ChatML格式\n",
    "dpo_ds = raw_ds.map(build_dpo_chatml, remove_columns=raw_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 181
   },
   "outputs": [],
   "source": [
    "dpo_ds = load_dataset(\"banghua/DL-DPO-Dataset\", split=\"train\")\n",
    "\n",
    "#设置Pandas显示选项以便更好地查看数据\n",
    "pd.set_option(\"display.max_colwidth\", None)  # 显示每个单元的完整内容\n",
    "pd.set_option(\"display.width\", 0)      # 自动调整宽度以适应内容\n",
    "\n",
    "#显示数据集的前5行\n",
    "sample_df = dpo_ds.select(range(5)).to_pandas()\n",
    "display(sample_df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484a3a7-a3c9-4b12-96ab-522a1a1f98b8",
   "metadata": {},
   "source": [
    "## DPO训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ba4e0",
   "metadata": {},
   "source": [
    "现在我们已经完成了数据部分，让我们开始真正的 DPO 训练。\n",
    "\n",
    "首先，如果我们不使用 GPU，为了加速这个过程，只取前 100 个样本。我们还需要 DPO 配置`DPOConfig`。\n",
    "\n",
    "现在，类似于我们为 `SFT`（监督微调）配置的设置，我们拥有类似的超参数：每个设备的训练批次大小`per_device_train_batch_size`、梯度累积步数`gradient_accumulation_steps`、训练轮数`num_train_epochs`、学习率`learning_rate`和日志记录步数`logging_steps`。所有这些都与 SFT 配置相同，除了一个新的超参数 `beta (β)`。\n",
    "\n",
    "正如我们在 DPO 原始公式中讨论过的，`beta` 本质上是一个超参数，它决定了对数差异`log differences`的重要性程度。这是一个重要的超参数，你可能需要与学习率一起调整，以获得最佳的 DPO 性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d5896-6fd6-43d2-85f1-dacbd594f4cf",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "if not USE_GPU:\n",
    "    dpo_ds = dpo_ds.select(range(100))\n",
    "\n",
    "config = DPOConfig(\n",
    "    beta=0.2, # beta参数控制选择和拒绝响应的权重\n",
    "    per_device_train_batch_size=1,# 每个设备的训练批次大小\n",
    "    gradient_accumulation_steps=8,# 梯度累积步数\n",
    "    num_train_epochs=1,# 训练的总轮数\n",
    "    learning_rate=5e-5,# 学习率\n",
    "    logging_steps=2,# 日志记录步数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566f166",
   "metadata": {},
   "source": [
    "现在，我们有了配置`config`和数据集`dataset`，准备就绪，可以开始训练了。我们启动 DPO 训练：\n",
    "\n",
    "首先，将 `model` 设置为我们在此加载的模型。\n",
    "\n",
    "对于 `reference_model`（参考模型），我们通常将其设置为 None，这样它会自动创建原始模型的副本作为参考模型并冻结其权重。\n",
    "\n",
    "- `args` 将是我们之前设置的配置。\n",
    "\n",
    "- `tokenizer` 是分词器。\n",
    "\n",
    "- `train_dataset` 是我们之前使用的 DPO 数据集。\n",
    "\n",
    "我们现在准备好训练了。如你所见，我们在一个 epoch 内总共训练了 100 个样本。我们设置的批次大小`batch_size`为 8。因此，我们仍然需要一定的步数来完成 DPO 过程。\n",
    "\n",
    "如前所述，由于我们在一个更小的模型上使用一个只将“Qwen”改为“Deep Qwen”的更小的数据集进行训练，所以这种训练不期望能达到我之前展示的（在更大模型和更大数据集上训练的）相同效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c0bba-984a-494c-8374-33db30ad1da6",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "#创建DPO训练器\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,# 模型\n",
    "    ref_model=None,# 参考模型（如果有的话）\n",
    "    args=config,    # 训练参数配置\n",
    "    processing_class=tokenizer,  # 分词器\n",
    "    train_dataset=dpo_ds# 训练数据集\n",
    ")\n",
    "#训练DPO模型\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef12da5",
   "metadata": {},
   "source": [
    "现在，DPO 训练已经在一个更小的模型和更小的数据集上完成，旨在将其行为身份从“Qwen”更改为“Deep Qwen”。\n",
    "\n",
    "你会看到，在此类训练之后，Qwen 的输出将其身份更改为“Deep Qwen”，而其他内容（包括其开发者、知识等）不会被改变。\n",
    "\n",
    "所以，可以将这里提到的完全训练好的 Qwen 模型视为小模型在小数据集上训练的快速验证结果，这样我们就有机会看到完整的 DPO 训练过程，而无需在有限的计算资源上等待太久。\n",
    "<hr>\n",
    "\n",
    "**Note:** 由于计算资源有限，我们使用了小型模型和数据集进行 DPO 训练。但以下展示的结果来自完整训练的大模型——`Qwen2.5-0.5B`，用于演示 DPO 流程的最终效果。如需查看小型模型和数据集的结果，请将参数 `fully_trained_qwen` 设置为 `False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521a00a-88d8-4ad9-b134-12ea94e76984",
   "metadata": {
    "height": 198
   },
   "outputs": [],
   "source": [
    "fully_trained_qwen = True\n",
    "if fully_trained_qwen:\n",
    "    model, qwen_tokenizer = load_model_and_tokenizer(\"./models/banghua/Qwen2.5-0.5B-DPO\", \n",
    "                                            USE_GPU)\n",
    "    test_model_with_questions(model, qwen_tokenizer, questions,\n",
    "                          title=\"Post-trained Model (After DPO) Output\")\n",
    "    del model, qwen_tokenizer\n",
    "else:\n",
    "    test_model_with_questions(dpo_trainer.model, tokenizer, questions,\n",
    "                          title=\"Post-trained Model (After DPO) Output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
