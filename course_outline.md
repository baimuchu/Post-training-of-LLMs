# Post-training of LLMs 课程大纲

| 课程名称 | 类型 | 时长 | 说明 |
|---------|------|------|------|
| Introduction | Video | 3 mins | 课程介绍 |
| Introduction to Post-training | Video | 9 mins | 后训练技术介绍 |
| Basics of SFT | Video | 8 mins | 监督微调基础理论 |
| SFT in Practice | Video with code | 13 mins | 监督微调实践（对应Lesson_3.ipynb） |
| Basics of DPO | Video | 7 mins | 直接偏好优化基础理论 |
| DPO in Practice | Video with code | 9 mins | 直接偏好优化实践（对应Lesson_5.ipynb） |
| Basics of Online RL | Video | 11 mins | 在线强化学习基础理论 |
| Online RL in Practice | Video with code | 11 mins | 在线强化学习实践（对应Lesson_7.ipynb） |
| Conclusion | Video | 2 mins | 课程总结 |
| Quiz | Reading | 1 min | 课程测验 |
| Appendix – Tips, Help, and Download | Code examples | 1 min | 附录和帮助（对应Appendix.ipynb） |

## 课程技术要点

- **SFT (Supervised Fine-Tuning)**: 监督微调，让模型学会遵循指令
- **DPO (Direct Preference Optimization)**: 直接偏好优化，模型与人类偏好对齐
- **Online RL**: 在线强化学习，针对特定任务持续优化

## 实践文件说明

- `Lesson_3.ipynb`: SFT实践代码
- `Lesson_5.ipynb`: DPO实践代码  
- `Lesson_7.ipynb`: Online RL实践代码
- `Appendix.ipynb`: 使用指南和学习资源 